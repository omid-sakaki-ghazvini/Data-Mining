{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqBFKlyHBnRDlZlJzXxUBG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0aqT7zJQzk0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install shap numpy pandas matplotlib scikit-learn lightgbm tensorflow torch transformers plotly wordcloud ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from ipywidgets import interact\n",
        "import shap\n",
        "shap.initjs()  # For better visualizations\n",
        "\n",
        "# Scikit-learn models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "\n",
        "# TensorFlow/Keras for CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Transformers (BERT)\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch"
      ],
      "metadata": {
        "id": "pFj4SEFjRaox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Tabular Data Models (Scikit-learn, XGBoost, LightGBM)**"
      ],
      "metadata": {
        "id": "VPG4lGUon4oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# SHAP interpretation\n",
        "explainer = shap.LinearExplainer(model, X_train)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Interactive summary plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=data.feature_names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCye3LAlRbij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "from xgboost import XGBClassifier\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# SHAP interpretation\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Interactive summary plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=data.feature_names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7b0dy4SARdbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Deep Learning for Images (Keras CNN, MNIST)**"
      ],
      "metadata": {
        "id": "qNbwT4L7oMAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Build a simple CNN\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=2, batch_size=64)\n",
        "\n",
        "# Select a subset for SHAP (computationally expensive)\n",
        "background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]\n",
        "test_images = X_test[:5]\n",
        "\n",
        "# SHAP Explanation\n",
        "explainer = shap.GradientExplainer(model, background)\n",
        "shap_values = explainer.shap_values(test_images)\n",
        "\n",
        "# Visualize pixel contributions\n",
        "shap.image_plot(shap_values, test_images)"
      ],
      "metadata": {
        "id": "j27oyHYjRf2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. NLP with Transformers (BERT/DistilBERT)**"
      ],
      "metadata": {
        "id": "cksuyffsoT2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Model and Tokenizer\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# 2. Robust Prediction Function\n",
        "def predict(texts):\n",
        "    # Convert all inputs to list format\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    elif isinstance(texts, np.ndarray):\n",
        "        texts = texts.tolist()\n",
        "\n",
        "    # Process each text separately\n",
        "    results = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        probas = torch.softmax(outputs.logits, dim=-1).numpy()\n",
        "        results.append(probas[0])  # Take only the first result\n",
        "    return np.array(results)\n",
        "\n",
        "# 3. Sample Texts for Analysis\n",
        "texts = [\n",
        "    \"The movie was fantastic with brilliant acting but the ending was disappointing\",\n",
        "    \"I've never seen such a terrible performance from the lead actor\",\n",
        "    \"The plot was predictable yet somehow still enjoyable\"\n",
        "]\n",
        "\n",
        "# 4. Create SHAP Explainer with Improved Method\n",
        "explainer = shap.Explainer(\n",
        "    model=predict,\n",
        "    masker=tokenizer,\n",
        "    output_names=[\"Negative\", \"Positive\"],\n",
        "    algorithm=\"permutation\"  # Changed algorithm for better compatibility\n",
        ")\n",
        "\n",
        "# Calculate SHAP values (start with just one example)\n",
        "shap_values = explainer(texts[:1])  # First try with just one text\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Advanced Visualization Techniques (after successful SHAP calculation)\n",
        "# --------------------------------------------------\n",
        "\n",
        "# 1. Basic Visualization to Start\n",
        "shap.plots.text(shap_values)\n",
        "\n",
        "# 2. Word Cloud of Feature Impacts (after successful initial calculation)\n",
        "def plot_word_cloud(shap_values, class_idx=1):\n",
        "    pos_words = {}\n",
        "    neg_words = {}\n",
        "\n",
        "    for i in range(len(shap_values.values)):\n",
        "        for j in range(len(shap_values[i].values)):\n",
        "            word = shap_values[i].data[j]\n",
        "            val = shap_values[i].values[j][class_idx]\n",
        "\n",
        "            if val > 0:\n",
        "                pos_words[word] = abs(val)\n",
        "            else:\n",
        "                neg_words[word] = abs(val)\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(pos_words)\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.title(\"Positive Impact Words\", fontsize=15)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    wc = WordCloud(width=800, height=400, background_color='black').generate_from_frequencies(neg_words)\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.title(\"Negative Impact Words\", fontsize=15)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Only execute if SHAP values were calculated successfully\n",
        "if 'shap_values' in locals():\n",
        "    try:\n",
        "        plot_word_cloud(shap_values)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in word cloud: {e}\")"
      ],
      "metadata": {
        "id": "CsJxa-_3Uou9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}